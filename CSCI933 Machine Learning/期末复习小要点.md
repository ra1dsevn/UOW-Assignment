kmeans肘部，选k

Given the clustering result, calculate the SSE the sum SSE get distortion graph

When the curve's descrese rate vary dramatically, the corresponding k is the best k.



贝叶斯

朴素贝叶斯：conditional independance

最大似然估计与贝叶斯

Fixed theta to find maximum likelihood

Unfixed theta, treat it as random parameter, and utilize prior probability, posterior to estimate parameter and quantify uncertainty.

线性可分：Perfectly separated.



Lasso: L1 penalty:

Absolute

Coefficients may be offset by the zero item in calculation, shrink Coefficients

Ridge Regression: L2 penalty

Square

Coefficients are reduced but not set to zero, all feature retained.

Simply by eliminating feature.

多重共线性（Multicollinearity）



LDA Linear Discrimination Analysis, projection. same close, unsame distant.





SVM: 

 ϵ: 每个变量的

C: sigma of ϵ,容忍度，越大support vector越宽

determine decision boundary, make the model robust to outliers



One2one

one2rest

Multi



![Pasted Graphic.png](blob:file:///38bf36cf-0116-44a4-b652-7081e613eb66)

精准率：预测中猜对了多少

召回率：正的猜到了多少



local optimisation 



生成式模型

要学习什么东西长什么样，有了长相在判别区分

Bayes network, 朴素贝叶斯，knn，GNN

判别式模型

直接学习特征差异

SVM, Linear Regression, NN.



降低维度

Lasso

PCA

LDA



Gradient descent algorithm, optimization algorithm, minimize loss function improvement. 

密度估计

头对头 weiduiwei





BP



The purpose is to design a classifier machine to classify through the pattern features learned/extracted from training data.

Representation pattern is the raw data collected from sensors.

Feature pattern refers to the features extracted from the raw data. It involves selecting important/effective features for classification and may include feature transformation.

The trained classifier uses the feature pattern to make decisions via pattern recognition.



Extend Two-class to Multi Class:

One2One

One2Rest

Hierarchical Tree based method

Each node can be viewed as a binary classifier or a multi-classifier, and hierarchical classification methods solve complex classification problems by cascading multiple classifiers trained independently.



CNNs are deep learning models primarily used for processing grid-like data, such as images, by extracting and learning spatial features through convolutional and pooling layers.



Shallow unsupervised learning refers to the use of machine learning techniques that involve minimal layering and no explicit supervision to discover patterns or structures in data, often for tasks like clustering or dimensionality reduction.



GANs are generative models that consist of a generator and a discriminator, trained adversarially to generate realistic data samples by learning the underlying data distribution.



RNNs, or Recurrent Neural Networks, are neural network models designed to effectively process sequential data by maintaining hidden states that capture temporal dependencies, allowing them to capture context and history in the data.



Pooling is a downsampling technique in convolutional neural networks (CNNs) used to reduce the spatial dimensions of feature maps while retaining important information. It helps to extract dominant features by summarizing local information and reducing computational complexity. Common pooling methods include max pooling, average pooling, and global pooling.